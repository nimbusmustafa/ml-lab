{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (ID3):\n",
      "[Feature: Temp.]\n",
      "  Value = 85:\n",
      "    Leaf: No\n",
      "  Value = 80:\n",
      "    Leaf: No\n",
      "  Value = 83:\n",
      "    Leaf: Yes\n",
      "  Value = 70:\n",
      "    Leaf: Yes\n",
      "  Value = 68:\n",
      "    Leaf: Yes\n",
      "  Value = 65:\n",
      "    Leaf: No\n",
      "  Value = 64:\n",
      "    Leaf: Yes\n",
      "  Value = 72:\n",
      "    [Feature: Outlook]\n",
      "      Value = Sunny:\n",
      "        Leaf: No\n",
      "      Value = Overcast:\n",
      "        Leaf: Yes\n",
      "  Value = 69:\n",
      "    Leaf: Yes\n",
      "  Value = 75:\n",
      "    Leaf: Yes\n",
      "  Value = 81:\n",
      "    Leaf: Yes\n",
      "  Value = 71:\n",
      "    Leaf: No\n",
      "\n",
      "Classification of new sample (ID3): No\n",
      "\n",
      "Decision Tree (C4.5):\n",
      "[Feature: Temp.]\n",
      "  Value = 85:\n",
      "    Leaf: No\n",
      "  Value = 80:\n",
      "    Leaf: No\n",
      "  Value = 83:\n",
      "    Leaf: Yes\n",
      "  Value = 70:\n",
      "    Leaf: Yes\n",
      "  Value = 68:\n",
      "    Leaf: Yes\n",
      "  Value = 65:\n",
      "    Leaf: No\n",
      "  Value = 64:\n",
      "    Leaf: Yes\n",
      "  Value = 72:\n",
      "    [Feature: Outlook]\n",
      "      Value = Sunny:\n",
      "        Leaf: No\n",
      "      Value = Overcast:\n",
      "        Leaf: Yes\n",
      "  Value = 69:\n",
      "    Leaf: Yes\n",
      "  Value = 75:\n",
      "    Leaf: Yes\n",
      "  Value = 81:\n",
      "    Leaf: Yes\n",
      "  Value = 71:\n",
      "    Leaf: No\n",
      "\n",
      "Classification of new sample (C4.5): No\n",
      "\n",
      "Decision Tree (CART):\n",
      "[Feature: Temp.]\n",
      "  Value = 85:\n",
      "    Leaf: No\n",
      "  Value = 80:\n",
      "    Leaf: No\n",
      "  Value = 83:\n",
      "    Leaf: Yes\n",
      "  Value = 70:\n",
      "    Leaf: Yes\n",
      "  Value = 68:\n",
      "    Leaf: Yes\n",
      "  Value = 65:\n",
      "    Leaf: No\n",
      "  Value = 64:\n",
      "    Leaf: Yes\n",
      "  Value = 72:\n",
      "    [Feature: Outlook]\n",
      "      Value = Sunny:\n",
      "        Leaf: No\n",
      "      Value = Overcast:\n",
      "        Leaf: Yes\n",
      "  Value = 69:\n",
      "    Leaf: Yes\n",
      "  Value = 75:\n",
      "    Leaf: Yes\n",
      "  Value = 81:\n",
      "    Leaf: Yes\n",
      "  Value = 71:\n",
      "    Leaf: No\n",
      "\n",
      "Classification of new sample (CART): No\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Sample dataset based on provided data\n",
    "data = [\n",
    "    {\"Outlook\": \"Sunny\", \"Temp.\": 85, \"Humidity\": 85, \"Wind\": \"Weak\", \"Decision\": \"No\"},\n",
    "    {\"Outlook\": \"Sunny\", \"Temp.\": 80, \"Humidity\": 90, \"Wind\": \"Strong\", \"Decision\": \"No\"},\n",
    "    {\"Outlook\": \"Overcast\", \"Temp.\": 83, \"Humidity\": 78, \"Wind\": \"Weak\", \"Decision\": \"Yes\"},\n",
    "    {\"Outlook\": \"Rain\", \"Temp.\": 70, \"Humidity\": 96, \"Wind\": \"Weak\", \"Decision\": \"Yes\"},\n",
    "    {\"Outlook\": \"Rain\", \"Temp.\": 68, \"Humidity\": 80, \"Wind\": \"Weak\", \"Decision\": \"Yes\"},\n",
    "    {\"Outlook\": \"Rain\", \"Temp.\": 65, \"Humidity\": 70, \"Wind\": \"Strong\", \"Decision\": \"No\"},\n",
    "    {\"Outlook\": \"Overcast\", \"Temp.\": 64, \"Humidity\": 65, \"Wind\": \"Strong\", \"Decision\": \"Yes\"},\n",
    "    {\"Outlook\": \"Sunny\", \"Temp.\": 72, \"Humidity\": 95, \"Wind\": \"Weak\", \"Decision\": \"No\"},\n",
    "    {\"Outlook\": \"Sunny\", \"Temp.\": 69, \"Humidity\": 70, \"Wind\": \"Weak\", \"Decision\": \"Yes\"},\n",
    "    {\"Outlook\": \"Rain\", \"Temp.\": 75, \"Humidity\": 80, \"Wind\": \"Weak\", \"Decision\": \"Yes\"},\n",
    "    {\"Outlook\": \"Sunny\", \"Temp.\": 75, \"Humidity\": 70, \"Wind\": \"Strong\", \"Decision\": \"Yes\"},\n",
    "    {\"Outlook\": \"Overcast\", \"Temp.\": 72, \"Humidity\": 90, \"Wind\": \"Strong\", \"Decision\": \"Yes\"},\n",
    "    {\"Outlook\": \"Overcast\", \"Temp.\": 81, \"Humidity\": 75, \"Wind\": \"Weak\", \"Decision\": \"Yes\"},\n",
    "    {\"Outlook\": \"Rain\", \"Temp.\": 71, \"Humidity\": 80, \"Wind\": \"Strong\", \"Decision\": \"No\"}\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Helper functions to calculate entropy, information gain, and split information\n",
    "def label_frequency(data):\n",
    "    label_counts = {}\n",
    "    for label in data[\"Decision\"]:\n",
    "        if label not in label_counts:\n",
    "            label_counts[label] = 0\n",
    "        label_counts[label] += 1\n",
    "    return label_counts\n",
    "\n",
    "def entropy(data):\n",
    "    label_counts = label_frequency(data)\n",
    "    total_instances = len(data)\n",
    "    entropy_value = 0\n",
    "    for count in label_counts.values():\n",
    "        probability = count / total_instances\n",
    "        entropy_value -= probability * math.log2(probability)\n",
    "    return entropy_value\n",
    "\n",
    "def information_gain(data, attribute):\n",
    "    total_entropy = entropy(data)\n",
    "    attribute_values = data[attribute].unique()\n",
    "    weighted_entropy = 0\n",
    "    for value in attribute_values:\n",
    "        subset = data[data[attribute] == value]\n",
    "        subset_entropy = entropy(subset)\n",
    "        weighted_entropy += (len(subset) / len(data)) * subset_entropy\n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "# Calculate split information (intrinsic value)\n",
    "def split_information(data, attribute):\n",
    "    total_instances = len(data)\n",
    "    attribute_values = data[attribute].unique()\n",
    "    split_info = 0\n",
    "    for value in attribute_values:\n",
    "        subset = data[data[attribute] == value]\n",
    "        probability = len(subset) / total_instances\n",
    "        split_info -= probability * math.log2(probability) if probability > 0 else 0\n",
    "    return split_info\n",
    "\n",
    "# Calculate gain ratio for C4.5\n",
    "def gain_ratio(data, attribute):\n",
    "    info_gain = information_gain(data, attribute)\n",
    "    split_info = split_information(data, attribute)\n",
    "    return info_gain / split_info if split_info != 0 else 0  # Avoid division by zero\n",
    "\n",
    "def gini_impurity(data):\n",
    "    label_counts = label_frequency(data)\n",
    "    total_instances = len(data)\n",
    "    gini = 1\n",
    "    for count in label_counts.values():\n",
    "        probability = count / total_instances\n",
    "        gini -= probability ** 2\n",
    "    return gini\n",
    "\n",
    "def gini_gain(data, attribute):\n",
    "    total_gini = gini_impurity(data)\n",
    "    attribute_values = data[attribute].unique()\n",
    "    weighted_gini = 0\n",
    "    for value in attribute_values:\n",
    "        subset = data[data[attribute] == value]\n",
    "        subset_gini = gini_impurity(subset)\n",
    "        weighted_gini += (len(subset) / len(data)) * subset_gini\n",
    "    return total_gini - weighted_gini\n",
    "\n",
    "# Function to find the best attribute based on selected criterion (ID3, C4.5, CART)\n",
    "def best_split(data, attributes, criterion=\"ID3\"):\n",
    "    best_gain = -1\n",
    "    best_attribute = None\n",
    "    for attribute in attributes:\n",
    "        if criterion == \"ID3\":\n",
    "            gain = information_gain(data, attribute)\n",
    "        elif criterion == \"C4.5\":\n",
    "            gain = gain_ratio(data, attribute)\n",
    "        elif criterion == \"CART\":\n",
    "            gain = gini_gain(data, attribute)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid criterion specified. Choose 'ID3', 'C4.5', or 'CART'.\")\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_attribute = attribute\n",
    "    return best_attribute\n",
    "\n",
    "# Recursive function to build the decision tree\n",
    "def build_tree(data, attributes, criterion=\"ID3\"):\n",
    "    # If all target labels are the same, return that label\n",
    "    if len(data[\"Decision\"].unique()) == 1:\n",
    "        return data[\"Decision\"].iloc[0]\n",
    "    \n",
    "    # If there are no more attributes to split, return the most common label\n",
    "    if len(attributes) == 0:\n",
    "        return data[\"Decision\"].mode()[0]\n",
    "    \n",
    "    # Find the best attribute to split on based on the criterion\n",
    "    best_attr = best_split(data, attributes, criterion)\n",
    "    tree = {best_attr: {}}\n",
    "    remaining_attributes = [attr for attr in attributes if attr != best_attr]\n",
    "    \n",
    "    # Split on each possible value of the best attribute\n",
    "    for value in data[best_attr].unique():\n",
    "        subset = data[data[best_attr] == value]\n",
    "        subtree = build_tree(subset, remaining_attributes, criterion)\n",
    "        tree[best_attr][value] = subtree\n",
    "    return tree\n",
    "\n",
    "# Function to classify a new sample\n",
    "def classify(tree, sample):\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    attribute = next(iter(tree))\n",
    "    value = sample.get(attribute)\n",
    "    if value not in tree[attribute]:\n",
    "        return \"Unknown\"\n",
    "    subtree = tree[attribute][value]\n",
    "    return classify(subtree, sample)\n",
    "\n",
    "# Pretty print function to display the tree structure\n",
    "def print_tree(tree, indent=\"\"):\n",
    "    if not isinstance(tree, dict):\n",
    "        print(indent + \"Leaf:\", tree)\n",
    "    else:\n",
    "        for attribute, branches in tree.items():\n",
    "            print(indent + f\"[Feature: {attribute}]\")\n",
    "            for value, subtree in branches.items():\n",
    "                print(indent + f\"  Value = {value}:\")\n",
    "                print_tree(subtree, indent + \"    \")\n",
    "\n",
    "# Build the decision trees using ID3, C4.5, and CART\n",
    "attributes = [\"Outlook\", \"Temp.\", \"Humidity\", \"Wind\"]\n",
    "decision_tree_id3 = build_tree(df, attributes, criterion=\"ID3\")\n",
    "decision_tree_c4_5 = build_tree(df, attributes, criterion=\"C4.5\")\n",
    "decision_tree_cart = build_tree(df, attributes, criterion=\"CART\")\n",
    "\n",
    "# Sample for testing the classification\n",
    "new_sample = {\"Outlook\": \"Sunny\", \"Temp.\": 72, \"Humidity\": 90, \"Wind\": \"Weak\"}\n",
    "classification_id3 = classify(decision_tree_id3, new_sample)\n",
    "classification_c4_5 = classify(decision_tree_c4_5, new_sample)\n",
    "classification_cart = classify(decision_tree_cart, new_sample)\n",
    "\n",
    "# Display the results\n",
    "print(\"Decision Tree (ID3):\")\n",
    "print_tree(decision_tree_id3)\n",
    "print(\"\\nClassification of new sample (ID3):\", classification_id3)\n",
    "\n",
    "print(\"\\nDecision Tree (C4.5):\")\n",
    "print_tree(decision_tree_c4_5)\n",
    "print(\"\\nClassification of new sample (C4.5):\", classification_c4_5)\n",
    "\n",
    "print(\"\\nDecision Tree (CART):\")\n",
    "print_tree(decision_tree_cart)\n",
    "print(\"\\nClassification of new sample (CART):\", classification_cart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree (C4.5): 100.00%\n",
      "Accuracy of Decision Tree (CART): 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate accuracy of the model\n",
    "def calculate_accuracy(tree, data):\n",
    "    correct_predictions = 0\n",
    "    for _, row in data.iterrows():\n",
    "        sample = row.to_dict()\n",
    "        true_label = sample[\"Decision\"]\n",
    "        predicted_label = classify(tree, sample)\n",
    "        if predicted_label == true_label:\n",
    "            correct_predictions += 1\n",
    "    accuracy = correct_predictions / len(data) * 100  # Accuracy in percentage\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy for both C4.5 and CART decision trees\n",
    "accuracy_c4_5 = calculate_accuracy(decision_tree_c4_5, df)\n",
    "accuracy_cart = calculate_accuracy(decision_tree_cart, df)\n",
    "\n",
    "# Display the accuracy results\n",
    "print(f\"Accuracy of Decision Tree (C4.5): {accuracy_c4_5:.2f}%\")\n",
    "print(f\"Accuracy of Decision Tree (CART): {accuracy_cart:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (C4.5): {'Credit': {'Good': 'Yes', 'Bad': {'Income': {'Low': 'No', 'Medium': 'Yes', 'High': 'No'}}}}\n",
      "Classification of new sample (C4.5): Yes\n",
      "\n",
      "Decision Tree (CART): {'Credit': {'Good': 'Yes', 'Bad': {'Income': {'Low': 'No', 'Medium': 'Yes', 'High': 'No'}}}}\n",
      "Classification of new sample (CART): Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Sample dataset based on provided data\n",
    "data = [\n",
    "    {\"Income\": \"Low\", \"Credit\": \"Good\", \"Loan Approved\": \"Yes\"},\n",
    "    {\"Income\": \"Low\", \"Credit\": \"Bad\", \"Loan Approved\": \"No\"},\n",
    "    {\"Income\": \"Medium\", \"Credit\": \"Good\", \"Loan Approved\": \"Yes\"},\n",
    "    {\"Income\": \"Medium\", \"Credit\": \"Bad\", \"Loan Approved\": \"Yes\"},\n",
    "    {\"Income\": \"High\", \"Credit\": \"Good\", \"Loan Approved\": \"Yes\"},\n",
    "    {\"Income\": \"High\", \"Credit\": \"Bad\", \"Loan Approved\": \"No\"}\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to calculate frequency of labels\n",
    "def label_frequency(data):\n",
    "    label_counts = {}\n",
    "    for label in data[\"Loan Approved\"]:\n",
    "        if label not in label_counts:\n",
    "            label_counts[label] = 0\n",
    "        label_counts[label] += 1\n",
    "    return label_counts\n",
    "\n",
    "# Function to calculate entropy (for C4.5)\n",
    "def entropy(data):\n",
    "    label_counts = label_frequency(data)\n",
    "    total_instances = len(data)\n",
    "    entropy_value = 0\n",
    "    for count in label_counts.values():\n",
    "        probability = count / total_instances\n",
    "        entropy_value -= probability * math.log2(probability)\n",
    "    return entropy_value\n",
    "\n",
    "# Function to calculate Gini impurity (for CART)\n",
    "def gini_impurity(data):\n",
    "    label_counts = label_frequency(data)\n",
    "    total_instances = len(data)\n",
    "    gini = 1\n",
    "    for count in label_counts.values():\n",
    "        probability = count / total_instances\n",
    "        gini -= probability ** 2\n",
    "    return gini\n",
    "\n",
    "# Function to calculate information gain for C4.5\n",
    "def information_gain(data, attribute):\n",
    "    total_entropy = entropy(data)\n",
    "    attribute_values = data[attribute].unique()\n",
    "    weighted_entropy = 0\n",
    "    for value in attribute_values:\n",
    "        subset = data[data[attribute] == value]\n",
    "        subset_entropy = entropy(subset)\n",
    "        weighted_entropy += (len(subset) / len(data)) * subset_entropy\n",
    "    return total_entropy - weighted_entropy\n",
    "\n",
    "# Function to calculate Gini gain for CART\n",
    "def gini_gain(data, attribute):\n",
    "    total_gini = gini_impurity(data)\n",
    "    attribute_values = data[attribute].unique()\n",
    "    weighted_gini = 0\n",
    "    for value in attribute_values:\n",
    "        subset = data[data[attribute] == value]\n",
    "        subset_gini = gini_impurity(subset)\n",
    "        weighted_gini += (len(subset) / len(data)) * subset_gini\n",
    "    return total_gini - weighted_gini\n",
    "\n",
    "# Function to find the best attribute to split on based on selected criterion\n",
    "def best_split(data, attributes, criterion=\"C4.5\"):\n",
    "    best_gain = -1\n",
    "    best_attribute = None\n",
    "    for attribute in attributes:\n",
    "        if criterion == \"C4.5\":\n",
    "            gain = information_gain(data, attribute)\n",
    "        elif criterion == \"CART\":\n",
    "            gain = gini_gain(data, attribute)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid criterion specified. Choose 'C4.5' or 'CART'.\")\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_attribute = attribute\n",
    "    return best_attribute\n",
    "\n",
    "# Recursive function to build the decision tree\n",
    "def build_tree(data, attributes, criterion=\"C4.5\"):\n",
    "    # If all target labels are the same, return that label\n",
    "    if len(data[\"Loan Approved\"].unique()) == 1:\n",
    "        return data[\"Loan Approved\"].iloc[0]\n",
    "    \n",
    "    # If there are no more attributes to split, return the most common label\n",
    "    if len(attributes) == 0:\n",
    "        return data[\"Loan Approved\"].mode()[0]\n",
    "    \n",
    "    # Find the best attribute to split on\n",
    "    best_attr = best_split(data, attributes, criterion)\n",
    "    tree = {best_attr: {}}\n",
    "    remaining_attributes = [attr for attr in attributes if attr != best_attr]\n",
    "    \n",
    "    # Split on each possible value of the best attribute\n",
    "    for value in data[best_attr].unique():\n",
    "        subset = data[data[best_attr] == value]\n",
    "        subtree = build_tree(subset, remaining_attributes, criterion)\n",
    "        tree[best_attr][value] = subtree\n",
    "    return tree\n",
    "\n",
    "# Function to classify a new sample\n",
    "def classify(tree, sample):\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    attribute = next(iter(tree))\n",
    "    value = sample.get(attribute)\n",
    "    if value not in tree[attribute]:\n",
    "        return \"Unknown\"\n",
    "    subtree = tree[attribute][value]\n",
    "    return classify(subtree, sample)\n",
    "\n",
    "# Building the decision trees using both C4.5 and CART\n",
    "attributes = [\"Income\", \"Credit\"]\n",
    "decision_tree_c4_5 = build_tree(df, attributes, criterion=\"C4.5\")\n",
    "decision_tree_cart = build_tree(df, attributes, criterion=\"CART\")\n",
    "\n",
    "# Sample for testing the classification\n",
    "new_sample = {\"Income\": \"Medium\", \"Credit\": \"Good\"}\n",
    "classification_c4_5 = classify(decision_tree_c4_5, new_sample)\n",
    "classification_cart = classify(decision_tree_cart, new_sample)\n",
    "\n",
    "# Display the results\n",
    "print(\"Decision Tree (C4.5):\", decision_tree_c4_5)\n",
    "print(\"Classification of new sample (C4.5):\", classification_c4_5)\n",
    "print(\"\\nDecision Tree (CART):\", decision_tree_cart)\n",
    "print(\"Classification of new sample (CART):\", classification_cart)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
